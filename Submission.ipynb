{"cells":[{"metadata":{},"cell_type":"markdown","source":"**KAGGLE TEAM: keep_it_high (ranked 28)**\n**ΚΩΝΣΤΑΝΤΙΝΟΣ ΒΛΑΧΑΚΗΣ 3140025\n  ΧΡΗΣΤΟΣ ΣΑΛΙΑΜΠΟΥΚΟΣ 3140176\n  ΙΩΑΝΝΗΣ ΔΗΜΗΤΡΑΚΟΠΟΥΛΟΣ 3150033**"},{"metadata":{},"cell_type":"markdown","source":"**Περιγραφή υλοποίησης**\nΤο πρόβλημα το προσεγγίσαμε με 2 μοντέλα αλγορίθμων.\nΣτην πρώτη προσπάθεια χρησιμοποιήσαμε τον booster **XGBRegressor** \nΣτην δεύτερη προσπάθεια προσεγγίσαμε το πρόβλημα δημιουργώντας ένα **Νευρωνικό Δίκτυο**\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Importing the libraries \n\nimport keras\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom matplotlib import pyplot as plt\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport warnings \nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_log_error","execution_count":59,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load data\n\ndf_test = pd.read_csv(\"../input/test.csv\")\ndf_train = pd.read_csv(\"../input/train.csv\")","execution_count":60,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# One-hot encoding using dummies from panda dataframe\n# After one-hot encoding at the test.csv there was not the 4th column so we concatenated it \n\ndummy_fields = ['yr','season','mnth','weathersit']\nfor each in dummy_fields:\n    dummies = pd.get_dummies(df_train[each], prefix=each, drop_first=False)\n    df_train = pd.concat([df_train, dummies], axis=1)\n    \nfields_to_drop = ['yr','season','mnth','weathersit']\ndata = df_train.drop(fields_to_drop, axis=1)\n\ndummy_fields2 = ['season','mnth','yr','weathersit']\nfor each in dummy_fields:\n    dummies2 = pd.get_dummies(df_test[each], prefix=each, drop_first=False)\n    df_test = pd.concat([df_test, dummies2], axis=1)\n    \nfields_to_drop = ['yr','season','mnth','weathersit']\ndata2 = df_test.drop(fields_to_drop, axis=1)\n\ndata2['weathersit_4']= df_train['weathersit_4']\n","execution_count":61,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We split the main data\n\nX = data.drop(['casual', 'registered', 'cnt', 'atemp'], axis=1)\ny = data['cnt']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# score function\ndef rmsle_score(y_true, y_pred):\n    for i, y in enumerate(y_pred):\n        if y_pred[i] < 0:\n            y_pred[i] = 0\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))","execution_count":63,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**XGBRegressor**\nΣαν πρώτο μοντέλο χρησιμοποιήσαμε τον αλγόριθμο XGBoost. \nΣτην αρχή κάναμε one-hot encoding στα 4 features από πάνω γιατί είδαμε οτί αυτά τα 4 δίνανε το βέλτιστο αποτέσμα.\nΚάναμε hyperparameter tuning για να βρούμε ποια νούμερα αποδίδουν καλύτερα στην κάθε μεταβλητή και τα εκχωρήσαμε στον πινακα parameters.\nΥλοποιήσαμε cross validation 5 φορές με την βοήθεια του GridSearchCV.\nΣτη συνέχεια κάναμε fit το μοντέλο μας με τα data που είχαν γινει split.\nΗ αλήθεια είναι πως ο αλγόριθμος αυτός αν και αποδείχθηκε πιο αποδοτικός απο κάθε άλλο regression αλγόριθμο είχε μεγάλο χρονο εκπαίδευσης της ταξη 2 με 2,5 λεπτά.\nΣτο καλύτερο αποτέλεσμα που οδηγηθήκαμε ήτανε το 0,326."},{"metadata":{"trusted":true},"cell_type":"code","source":"rf= XGBRegressor()\nparameters = {\n 'max_depth': [35],\n 'max_features': ['auto'],\n 'max_leaf_nodes': [None],\n 'min_impurity_decrease': [0.0],\n 'min_impurity_split': [None],\n 'min_samples_leaf': [1],\n 'min_samples_split': [3],\n 'min_weight_fraction_leaf': [0.0],\n 'n_estimators': [800],\n 'random_state': [42],\n  }","execution_count":64,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_cv = GridSearchCV(rf, parameters, cv=5, n_jobs=-1)\nrf_cv.fit(X_train, y_train)","execution_count":65,"outputs":[{"output_type":"stream","text":"[14:43:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n","name":"stdout"},{"output_type":"execute_result","execution_count":65,"data":{"text/plain":"GridSearchCV(cv=5, error_score='raise-deprecating',\n             estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n                                    colsample_bylevel=1, colsample_bynode=1,\n                                    colsample_bytree=1, gamma=0,\n                                    importance_type='gain', learning_rate=0.1,\n                                    max_delta_step=0, max_depth=3,\n                                    min_child_weight=1, missing=None,\n                                    n_estimators=100, n_jobs=1, nthread=None,\n                                    objective='reg:linear', random_state=0,...\n             iid='warn', n_jobs=-1,\n             param_grid={'max_depth': [35], 'max_features': ['auto'],\n                         'max_leaf_nodes': [None],\n                         'min_impurity_decrease': [0.0],\n                         'min_impurity_split': [None], 'min_samples_leaf': [1],\n                         'min_samples_split': [3],\n                         'min_weight_fraction_leaf': [0.0],\n                         'n_estimators': [800], 'random_state': [42]},\n             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n             scoring=None, verbose=0)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred= rf_cv.predict(X_test)\n\nprint('XGBRegressor RMSLE score:', rmsle_score(y_test, y_pred))","execution_count":66,"outputs":[{"output_type":"stream","text":"XGBRegressor RMSLE score: 0.32634668462245897\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Νευρωνικά Δίκτυα**\nΑυτή την φορά κάνουμε one-hot encoding σε όλα τα categorical features γιατί λειτουργούν καλύτερα τα ΝΝ με περισσότερα data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load data again\n\ndf_test = pd.read_csv(\"../input/test.csv\")\ndf_train = pd.read_csv(\"../input/train.csv\")","execution_count":67,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One-hot encoding using dummies from panda dataframe\n# After one-hot encoding at the test.csv there was not the 4th column so we concatenated it \n\ndummy_fields = ['hr','yr','season','mnth','holiday','weekday','workingday','weathersit']\nfor each in dummy_fields:\n    dummies = pd.get_dummies(df_train[each], prefix=each, drop_first=False)\n    df_train = pd.concat([df_train, dummies], axis=1)\n    \nfields_to_drop = ['hr','yr','season','mnth','holiday','weekday','workingday','weathersit']\ndata = df_train.drop(fields_to_drop, axis=1)\n\ndummy_fields2 = ['hr','season','mnth','yr','holiday','weekday','workingday','weathersit']\nfor each in dummy_fields:\n    dummies2 = pd.get_dummies(df_test[each], prefix=each, drop_first=False)\n    df_test = pd.concat([df_test, dummies2], axis=1)\n    \nfields_to_drop = ['hr','yr','season','mnth','holiday','weekday','workingday','weathersit']\ndata2 = df_test.drop(fields_to_drop, axis=1)\n\ndata2['weathersit_4']= df_train['weathersit_4']\n\n","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(['casual', 'registered', 'cnt', 'atemp'], axis=1)\ny = data['cnt']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":69,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Εδώ ορίζουμε το Νευρωνικό μας δίκτυο.**\nΣτο input layer έχουμε 59 input parameters και αποφασίζουμε να χρησιμοποιήσουμε 64 input nodes.\nΣαν activation function επιλέξαμε την regression linear units.\nΈπειτα από αρκετές δοκιμές καταλήξαμε οτι βέλτιστο είναι να χρησιμοποιήσουμε 3 hidden layers των 64 nodes έκαστως και ξανα relu για activation function.\nΤέλος το ouput layer έχει 1 μεταβλητή και σαν activation το relu ξανά.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the neural network.\n\nNN_model = Sequential()\n# The Input Layer :\nNN_model.add(Dense(64, kernel_initializer='uniform',input_dim = X_train.shape[1], activation='relu'))\n\n# The Hidden Layers :\nNN_model.add(Dense(64, kernel_initializer='uniform',activation='relu'))\nNN_model.add(Dense(64, kernel_initializer='uniform',activation='relu'))\nNN_model.add(Dense(64, kernel_initializer='uniform',activation='relu'))\n\n\n# The Output Layer :\nNN_model.add(Dense(1,kernel_initializer='uniform', activation='relu'))\n","execution_count":70,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Optimizer**\nΣε αυτό το σημείο χρησιμοποήσαμε τον optimizer Adadelta με τις συγκεκριμένες παραμέτρους για learning rate και decay γιατί λειτούργησαν καλύτερα από RMSprop και Adam."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Optimizer for the compile function\nopt=  keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n\n# Compile the network :\nNN_model.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mean_absolute_error'],)","execution_count":71,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Γεμίζουμε το μοντέλο μας με τα split data και επιλέξαμε 40 φορές να τρέξει το ΝΝ  και μέγεθος sample 20 ώστε να αποφύγουμε under και overfitting. Ο αλγόριθμος χρειάζεται μόλις 40 δευτερόλεπτα για να κανει το train και το αποτέλεσμα του ειναι 0,31."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We fit the neural network with our split data from above\nNN_model.fit(X_train, y_train, epochs=40, batch_size=20)","execution_count":72,"outputs":[{"output_type":"stream","text":"Epoch 1/40\n9732/9732 [==============================] - 1s 98us/step - loss: 102.8040 - mean_absolute_error: 102.8040\nEpoch 2/40\n9732/9732 [==============================] - 1s 77us/step - loss: 57.5021 - mean_absolute_error: 57.5021\nEpoch 3/40\n9732/9732 [==============================] - 1s 78us/step - loss: 40.0974 - mean_absolute_error: 40.0974\nEpoch 4/40\n9732/9732 [==============================] - 1s 78us/step - loss: 32.9785 - mean_absolute_error: 32.9785\nEpoch 5/40\n9732/9732 [==============================] - 1s 78us/step - loss: 30.9636 - mean_absolute_error: 30.9637\nEpoch 6/40\n9732/9732 [==============================] - 1s 77us/step - loss: 29.9988 - mean_absolute_error: 29.9988\nEpoch 7/40\n9732/9732 [==============================] - 1s 77us/step - loss: 29.1298 - mean_absolute_error: 29.1298\nEpoch 8/40\n9732/9732 [==============================] - 1s 78us/step - loss: 28.4580 - mean_absolute_error: 28.4580\nEpoch 9/40\n9732/9732 [==============================] - 1s 77us/step - loss: 27.9546 - mean_absolute_error: 27.9546\nEpoch 10/40\n9732/9732 [==============================] - 1s 79us/step - loss: 27.3908 - mean_absolute_error: 27.3908\nEpoch 11/40\n9732/9732 [==============================] - 1s 77us/step - loss: 27.0456 - mean_absolute_error: 27.0456\nEpoch 12/40\n9732/9732 [==============================] - 1s 77us/step - loss: 26.6451 - mean_absolute_error: 26.6451\nEpoch 13/40\n9732/9732 [==============================] - 1s 79us/step - loss: 26.3662 - mean_absolute_error: 26.3662\nEpoch 14/40\n9732/9732 [==============================] - 1s 79us/step - loss: 26.0398 - mean_absolute_error: 26.0398\nEpoch 15/40\n9732/9732 [==============================] - 1s 77us/step - loss: 25.7667 - mean_absolute_error: 25.7667\nEpoch 16/40\n9732/9732 [==============================] - 1s 77us/step - loss: 25.5165 - mean_absolute_error: 25.5165\nEpoch 17/40\n9732/9732 [==============================] - 1s 77us/step - loss: 25.2749 - mean_absolute_error: 25.2750\nEpoch 18/40\n9732/9732 [==============================] - 1s 78us/step - loss: 25.0998 - mean_absolute_error: 25.0998\nEpoch 19/40\n9732/9732 [==============================] - 1s 77us/step - loss: 24.8188 - mean_absolute_error: 24.8188\nEpoch 20/40\n9732/9732 [==============================] - 1s 77us/step - loss: 24.5107 - mean_absolute_error: 24.5107\nEpoch 21/40\n9732/9732 [==============================] - 1s 77us/step - loss: 24.4064 - mean_absolute_error: 24.4064\nEpoch 22/40\n9732/9732 [==============================] - 1s 77us/step - loss: 24.2241 - mean_absolute_error: 24.2241\nEpoch 23/40\n9732/9732 [==============================] - 1s 76us/step - loss: 24.1017 - mean_absolute_error: 24.1017\nEpoch 24/40\n9732/9732 [==============================] - 1s 77us/step - loss: 23.9509 - mean_absolute_error: 23.9509\nEpoch 25/40\n9732/9732 [==============================] - 1s 78us/step - loss: 23.7138 - mean_absolute_error: 23.7138\nEpoch 26/40\n9732/9732 [==============================] - 1s 78us/step - loss: 23.5606 - mean_absolute_error: 23.5606\nEpoch 27/40\n9732/9732 [==============================] - 1s 77us/step - loss: 23.4500 - mean_absolute_error: 23.4500\nEpoch 28/40\n9732/9732 [==============================] - 1s 77us/step - loss: 23.2005 - mean_absolute_error: 23.2005\nEpoch 29/40\n9732/9732 [==============================] - 1s 78us/step - loss: 23.2402 - mean_absolute_error: 23.2402\nEpoch 30/40\n9732/9732 [==============================] - 1s 77us/step - loss: 23.1006 - mean_absolute_error: 23.1006\nEpoch 31/40\n9732/9732 [==============================] - 1s 78us/step - loss: 22.9050 - mean_absolute_error: 22.9050\nEpoch 32/40\n9732/9732 [==============================] - 1s 77us/step - loss: 22.8715 - mean_absolute_error: 22.8715\nEpoch 33/40\n9732/9732 [==============================] - 1s 77us/step - loss: 22.6471 - mean_absolute_error: 22.6471\nEpoch 34/40\n9732/9732 [==============================] - 1s 77us/step - loss: 22.5804 - mean_absolute_error: 22.5804\nEpoch 35/40\n9732/9732 [==============================] - 1s 77us/step - loss: 22.3914 - mean_absolute_error: 22.3914\nEpoch 36/40\n9732/9732 [==============================] - 1s 77us/step - loss: 22.3766 - mean_absolute_error: 22.3766\nEpoch 37/40\n9732/9732 [==============================] - 1s 76us/step - loss: 22.2016 - mean_absolute_error: 22.2016\nEpoch 38/40\n9732/9732 [==============================] - 1s 78us/step - loss: 22.0792 - mean_absolute_error: 22.0792\nEpoch 39/40\n9732/9732 [==============================] - 1s 77us/step - loss: 22.1168 - mean_absolute_error: 22.1168\nEpoch 40/40\n9732/9732 [==============================] - 1s 77us/step - loss: 21.9893 - mean_absolute_error: 21.9893\n","name":"stdout"},{"output_type":"execute_result","execution_count":72,"data":{"text/plain":"<keras.callbacks.callbacks.History at 0x7f6236fedba8>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred= NN_model.predict(X_test)\nprint('Neural Network RMSLE score:', rmsle_score(y_test, y_pred))","execution_count":73,"outputs":[{"output_type":"stream","text":"Neural Network RMSLE score: 0.3129279019717573\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Το μοντέλο του Νευρωνικού δικτύου σε 40 δευτερόλεπτα έκανε train και έχει έτοιμα τα predicted στοιχεία και με σκορ 0,31."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = NN_model.predict(data2.drop(['atemp'], axis=1))","execution_count":74,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Τελευταίο Στάδιο**\nΦτιάχνουμε τις δύο στήλες Id και Predicted και περνάμε μέσα στο dataframe τα νούμερα που βρήκε το ΝΝ."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['Id'] = range(predictions.shape[0])\nsubmission['Predicted'] = predictions\n\n\nfilename = 'Submission.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","execution_count":75,"outputs":[{"output_type":"stream","text":"Saved file: Submission.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Παρόλου που στις υποδείξεις για βελτίωση του μοντέλου μας ηταν το να κανουμε predict ξεχωριστα τα casual/registered δεν βοήθησε στην δική μας περίπτωση.**\nΔεν υλοποήσαμε κάποια τεχνική dimentional reduction και ίσως να βοηθούσε το μοντέλο μας σε ένα βαθμό. Επίσης χρησιμοποιήσαμε την τεχνική scaling ώστε να αναπαραστίσουμε τα δεδομένα μας σε διαστήμα 0-1 όμως δεν είδαμε καποια βελτίωση ή τελος παντων δεν ηταν ουσιαστική. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}